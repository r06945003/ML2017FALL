{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
    "import csv\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.layers import Embedding, Reshape, Merge, Dropout, Dense, add, Input, Add, Dot\n",
    "from keras.models import Sequential, Model, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unison_shuffled_copies(a, b, c):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p], c[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "899873\n"
     ]
    }
   ],
   "source": [
    "text = open('train.csv', 'r') \n",
    "row = csv.reader(text , delimiter=\"\\n\")\n",
    "count = 0\n",
    "user = []\n",
    "movie = []\n",
    "rating = []\n",
    "genres_train = []\n",
    "for r in row:\n",
    "    if count > 0:\n",
    "        r = r[0].split(',')\n",
    "        user.append(r[1])\n",
    "        movie.append(r[2])\n",
    "        #genres_train.append(m_genres[int(r[2])-1])\n",
    "        rating.append(r[3])\n",
    "    count = 1\n",
    "text.close()\n",
    "print(len(user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6040\n",
      "3952\n"
     ]
    }
   ],
   "source": [
    "user = np.array(user)\n",
    "movie = np.array(movie)\n",
    "rating = np.array(rating)\n",
    "#genres_train = np.array(genres_train)\n",
    "user=user.astype(np.int)\n",
    "movie=movie.astype(np.int)\n",
    "rating=rating.astype(np.int)\n",
    "#genres_train=genres_train.astype(np.int)\n",
    "\n",
    "max_userid = np.max(user)\n",
    "max_movieid = np.max(movie)\n",
    "print(max_userid)\n",
    "print(max_movieid)\n",
    "K_FACTORS = 120\n",
    "\n",
    "user = user - 1\n",
    "movie = movie - 1\n",
    "\n",
    "user, movie, rating = unison_shuffled_copies(user, movie, rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.58171208604\n",
      "1.11689766115\n",
      "[ 0.37450872  0.37450872  0.37450872 ...,  0.37450872  0.37450872\n",
      " -1.41616564]\n"
     ]
    }
   ],
   "source": [
    "#normalize\n",
    "#means = np.mean(rating)\n",
    "#std = np.std(rating)\n",
    "#rating = (rating - means)/std\n",
    "#print(means)\n",
    "#print(std)\n",
    "#print(rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def rmse(y_true, y_pred): return K.sqrt( K.mean((y_pred - y_true)**2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Input(shape=(1,))\n",
    "y = Input(shape=(1,))\n",
    "Z = Input(shape=(1,))\n",
    "embedding_layer_U = Embedding(max_userid, 256 , input_length=1)\n",
    "embedding_layer_M = Embedding(max_movieid, 256 , input_length=1)\n",
    "User = embedding_layer_U(x)\n",
    "#User = Reshape((120,))(User)\n",
    "User = Dropout(0.5)(Reshape((256,))(User))\n",
    "Movie = embedding_layer_M(y)\n",
    "#Movie =Reshape((120,))(Movie)\n",
    "Movie = Dropout(0.5)(Reshape((256,))(Movie))\n",
    "R = Dot(axes=1)([User, Movie])\n",
    "\n",
    "embedding_layer_U_bias = Embedding(max_userid, 1 , input_length=1, embeddings_initializer='zeros')\n",
    "embedding_layer_M_bias = Embedding(max_movieid, 1 , input_length=1, embeddings_initializer='zeros')\n",
    "embedding_layer_M_genres = Embedding(4, 1 , input_length=1, embeddings_initializer='zeros')\n",
    "\n",
    "Bias_U = embedding_layer_U_bias(x)\n",
    "Bias_U = Reshape((1,))(Bias_U)\n",
    "Bias_M = embedding_layer_M_bias(y)\n",
    "Bias_M = Reshape((1,))(Bias_M)\n",
    "\n",
    "Bias_genres = embedding_layer_M_genres(Z)\n",
    "Bias_genres = Reshape((1,))(Bias_genres)\n",
    "\n",
    "Bias = Add()([Bias_U, Bias_M])\n",
    "Bias = Add()([Bias, Bias_genres])\n",
    "\n",
    "final = Add()([R, Bias])\n",
    "model = Model([x, y], R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 854879 samples, validate on 44994 samples\n",
      "Epoch 1/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 14.0712 - rmse: 3.7511Epoch 00000: val_rmse improved from inf to 3.74353, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 12s - loss: 14.0710 - rmse: 3.7511 - val_loss: 14.0141 - val_rmse: 3.7435\n",
      "Epoch 2/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 13.3843 - rmse: 3.6574Epoch 00001: val_rmse improved from 3.74353 to 3.39474, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 12s - loss: 13.3743 - rmse: 3.6560 - val_loss: 11.5243 - val_rmse: 3.3947\n",
      "Epoch 3/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 6.8190 - rmse: 2.5573Epoch 00002: val_rmse improved from 3.39474 to 1.63995, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 12s - loss: 6.7964 - rmse: 2.5523 - val_loss: 2.6895 - val_rmse: 1.6400\n",
      "Epoch 4/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 1.9112 - rmse: 1.3761 ETA: 0s - loss: 1.9394 - rmEpoch 00003: val_rmse improved from 1.63995 to 1.11598, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 12s - loss: 1.9088 - rmse: 1.3752 - val_loss: 1.2454 - val_rmse: 1.1160\n",
      "Epoch 5/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 1.2763 - rmse: 1.1293Epoch 00004: val_rmse improved from 1.11598 to 0.99004, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 10s - loss: 1.2756 - rmse: 1.1290 - val_loss: 0.9802 - val_rmse: 0.9900\n",
      "Epoch 6/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 1.1263 - rmse: 1.0612Epoch 00005: val_rmse improved from 0.99004 to 0.94639, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 10s - loss: 1.1260 - rmse: 1.0611 - val_loss: 0.8957 - val_rmse: 0.9464\n",
      "Epoch 7/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 1.0711 - rmse: 1.0349Epoch 00006: val_rmse improved from 0.94639 to 0.92825, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 10s - loss: 1.0709 - rmse: 1.0348 - val_loss: 0.8617 - val_rmse: 0.9282\n",
      "Epoch 8/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 1.0416 - rmse: 1.0206Epoch 00007: val_rmse improved from 0.92825 to 0.91921, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 10s - loss: 1.0416 - rmse: 1.0205 - val_loss: 0.8450 - val_rmse: 0.9192\n",
      "Epoch 9/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 1.0256 - rmse: 1.0127Epoch 00008: val_rmse improved from 0.91921 to 0.91309, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 10s - loss: 1.0255 - rmse: 1.0126 - val_loss: 0.8338 - val_rmse: 0.9131\n",
      "Epoch 10/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 1.0109 - rmse: 1.0054Epoch 00009: val_rmse improved from 0.91309 to 0.90846, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 10s - loss: 1.0110 - rmse: 1.0054 - val_loss: 0.8253 - val_rmse: 0.9085\n",
      "Epoch 11/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.9994 - rmse: 0.9997Epoch 00010: val_rmse improved from 0.90846 to 0.90397, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 10s - loss: 0.9992 - rmse: 0.9996 - val_loss: 0.8172 - val_rmse: 0.9040\n",
      "Epoch 12/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.9869 - rmse: 0.9934Epoch 00011: val_rmse improved from 0.90397 to 0.90000, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 9s - loss: 0.9868 - rmse: 0.9933 - val_loss: 0.8100 - val_rmse: 0.9000\n",
      "Epoch 13/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.9758 - rmse: 0.9878Epoch 00012: val_rmse improved from 0.90000 to 0.89513, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 10s - loss: 0.9758 - rmse: 0.9878 - val_loss: 0.8013 - val_rmse: 0.8951\n",
      "Epoch 14/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.9664 - rmse: 0.9830Epoch 00013: val_rmse improved from 0.89513 to 0.89147, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 11s - loss: 0.9667 - rmse: 0.9832 - val_loss: 0.7947 - val_rmse: 0.8915\n",
      "Epoch 15/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.9563 - rmse: 0.9779Epoch 00014: val_rmse improved from 0.89147 to 0.88775, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 11s - loss: 0.9564 - rmse: 0.9779 - val_loss: 0.7881 - val_rmse: 0.8877\n",
      "Epoch 16/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.9483 - rmse: 0.9738Epoch 00015: val_rmse improved from 0.88775 to 0.88480, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 10s - loss: 0.9483 - rmse: 0.9738 - val_loss: 0.7829 - val_rmse: 0.8848\n",
      "Epoch 17/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.9401 - rmse: 0.9695Epoch 00016: val_rmse improved from 0.88480 to 0.88219, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 10s - loss: 0.9402 - rmse: 0.9696 - val_loss: 0.7783 - val_rmse: 0.8822\n",
      "Epoch 18/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.9340 - rmse: 0.9664Epoch 00017: val_rmse improved from 0.88219 to 0.87990, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 9s - loss: 0.9341 - rmse: 0.9664 - val_loss: 0.7742 - val_rmse: 0.8799\n",
      "Epoch 19/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.9294 - rmse: 0.9640Epoch 00018: val_rmse improved from 0.87990 to 0.87796, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 9s - loss: 0.9294 - rmse: 0.9640 - val_loss: 0.7708 - val_rmse: 0.8780\n",
      "Epoch 20/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.9226 - rmse: 0.9605Epoch 00019: val_rmse improved from 0.87796 to 0.87550, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 9s - loss: 0.9226 - rmse: 0.9605 - val_loss: 0.7665 - val_rmse: 0.8755\n",
      "Epoch 21/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.9185 - rmse: 0.9584Epoch 00020: val_rmse improved from 0.87550 to 0.87377, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 10s - loss: 0.9185 - rmse: 0.9584 - val_loss: 0.7635 - val_rmse: 0.8738\n",
      "Epoch 22/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.9117 - rmse: 0.9548Epoch 00021: val_rmse improved from 0.87377 to 0.87176, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 9s - loss: 0.9118 - rmse: 0.9548 - val_loss: 0.7600 - val_rmse: 0.8718\n",
      "Epoch 23/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.9085 - rmse: 0.9531Epoch 00022: val_rmse improved from 0.87176 to 0.87002, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 9s - loss: 0.9087 - rmse: 0.9532 - val_loss: 0.7570 - val_rmse: 0.8700\n",
      "Epoch 24/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.9028 - rmse: 0.9501Epoch 00023: val_rmse improved from 0.87002 to 0.86858, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 9s - loss: 0.9029 - rmse: 0.9502 - val_loss: 0.7544 - val_rmse: 0.8686\n",
      "Epoch 25/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.9013 - rmse: 0.9493Epoch 00024: val_rmse improved from 0.86858 to 0.86746, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 11s - loss: 0.9014 - rmse: 0.9494 - val_loss: 0.7525 - val_rmse: 0.8675\n",
      "Epoch 26/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8964 - rmse: 0.9468Epoch 00025: val_rmse improved from 0.86746 to 0.86582, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 9s - loss: 0.8965 - rmse: 0.9468 - val_loss: 0.7497 - val_rmse: 0.8658\n",
      "Epoch 27/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8942 - rmse: 0.9456Epoch 00026: val_rmse improved from 0.86582 to 0.86479, saving model to hw5_nobias.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "854879/854879 [==============================] - 9s - loss: 0.8942 - rmse: 0.9456 - val_loss: 0.7479 - val_rmse: 0.8648\n",
      "Epoch 28/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8916 - rmse: 0.9442Epoch 00027: val_rmse improved from 0.86479 to 0.86371, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 9s - loss: 0.8917 - rmse: 0.9442 - val_loss: 0.7460 - val_rmse: 0.8637\n",
      "Epoch 29/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8880 - rmse: 0.9423Epoch 00028: val_rmse improved from 0.86371 to 0.86278, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 9s - loss: 0.8882 - rmse: 0.9424 - val_loss: 0.7444 - val_rmse: 0.8628\n",
      "Epoch 30/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8854 - rmse: 0.9409Epoch 00029: val_rmse improved from 0.86278 to 0.86167, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 9s - loss: 0.8854 - rmse: 0.9409 - val_loss: 0.7425 - val_rmse: 0.8617\n",
      "Epoch 31/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8831 - rmse: 0.9397Epoch 00030: val_rmse improved from 0.86167 to 0.86130, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 9s - loss: 0.8832 - rmse: 0.9398 - val_loss: 0.7419 - val_rmse: 0.8613\n",
      "Epoch 32/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8825 - rmse: 0.9394Epoch 00031: val_rmse improved from 0.86130 to 0.86108, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 9s - loss: 0.8826 - rmse: 0.9394 - val_loss: 0.7415 - val_rmse: 0.8611\n",
      "Epoch 33/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8795 - rmse: 0.9378Epoch 00032: val_rmse improved from 0.86108 to 0.85970, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 9s - loss: 0.8797 - rmse: 0.9379 - val_loss: 0.7391 - val_rmse: 0.8597\n",
      "Epoch 34/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8780 - rmse: 0.9370Epoch 00033: val_rmse improved from 0.85970 to 0.85871, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 9s - loss: 0.8780 - rmse: 0.9370 - val_loss: 0.7374 - val_rmse: 0.8587\n",
      "Epoch 35/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8747 - rmse: 0.9353Epoch 00034: val_rmse improved from 0.85871 to 0.85869, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 9s - loss: 0.8748 - rmse: 0.9353 - val_loss: 0.7374 - val_rmse: 0.8587\n",
      "Epoch 36/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8741 - rmse: 0.9349Epoch 00035: val_rmse improved from 0.85869 to 0.85806, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 9s - loss: 0.8741 - rmse: 0.9349 - val_loss: 0.7363 - val_rmse: 0.8581\n",
      "Epoch 37/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8734 - rmse: 0.9345Epoch 00036: val_rmse improved from 0.85806 to 0.85766, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 9s - loss: 0.8734 - rmse: 0.9345 - val_loss: 0.7356 - val_rmse: 0.8577\n",
      "Epoch 38/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8690 - rmse: 0.9322Epoch 00037: val_rmse improved from 0.85766 to 0.85650, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 9s - loss: 0.8689 - rmse: 0.9321 - val_loss: 0.7336 - val_rmse: 0.8565\n",
      "Epoch 39/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8711 - rmse: 0.9333Epoch 00038: val_rmse improved from 0.85650 to 0.85638, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 9s - loss: 0.8712 - rmse: 0.9333 - val_loss: 0.7334 - val_rmse: 0.8564\n",
      "Epoch 40/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8673 - rmse: 0.9313Epoch 00039: val_rmse improved from 0.85638 to 0.85563, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 9s - loss: 0.8673 - rmse: 0.9313 - val_loss: 0.7321 - val_rmse: 0.8556\n",
      "Epoch 41/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8644 - rmse: 0.9297Epoch 00040: val_rmse improved from 0.85563 to 0.85529, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 9s - loss: 0.8644 - rmse: 0.9297 - val_loss: 0.7315 - val_rmse: 0.8553\n",
      "Epoch 42/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8642 - rmse: 0.9296Epoch 00041: val_rmse improved from 0.85529 to 0.85491, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 9s - loss: 0.8642 - rmse: 0.9296 - val_loss: 0.7309 - val_rmse: 0.8549\n",
      "Epoch 43/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8641 - rmse: 0.9295Epoch 00042: val_rmse did not improve\n",
      "854879/854879 [==============================] - 9s - loss: 0.8640 - rmse: 0.9295 - val_loss: 0.7310 - val_rmse: 0.8550\n",
      "Epoch 44/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8607 - rmse: 0.9277Epoch 00043: val_rmse improved from 0.85491 to 0.85427, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 9s - loss: 0.8607 - rmse: 0.9277 - val_loss: 0.7298 - val_rmse: 0.8543\n",
      "Epoch 45/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8602 - rmse: 0.9275Epoch 00044: val_rmse improved from 0.85427 to 0.85373, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 9s - loss: 0.8602 - rmse: 0.9275 - val_loss: 0.7289 - val_rmse: 0.8537\n",
      "Epoch 46/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8583 - rmse: 0.9264Epoch 00045: val_rmse improved from 0.85373 to 0.85292, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 9s - loss: 0.8584 - rmse: 0.9265 - val_loss: 0.7275 - val_rmse: 0.8529\n",
      "Epoch 47/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8579 - rmse: 0.9262Epoch 00046: val_rmse did not improve\n",
      "854879/854879 [==============================] - 9s - loss: 0.8579 - rmse: 0.9262 - val_loss: 0.7276 - val_rmse: 0.8530\n",
      "Epoch 48/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8566 - rmse: 0.9255Epoch 00047: val_rmse did not improve\n",
      "854879/854879 [==============================] - 9s - loss: 0.8566 - rmse: 0.9255 - val_loss: 0.7279 - val_rmse: 0.8531\n",
      "Epoch 49/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8544 - rmse: 0.9243Epoch 00048: val_rmse improved from 0.85292 to 0.85243, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 9s - loss: 0.8544 - rmse: 0.9243 - val_loss: 0.7267 - val_rmse: 0.8524\n",
      "Epoch 50/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8529 - rmse: 0.9235Epoch 00049: val_rmse improved from 0.85243 to 0.85230, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 9s - loss: 0.8530 - rmse: 0.9236 - val_loss: 0.7264 - val_rmse: 0.8523\n",
      "Epoch 51/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8549 - rmse: 0.9246 - ETA: 1s - loss:Epoch 00050: val_rmse did not improve\n",
      "854879/854879 [==============================] - 9s - loss: 0.8549 - rmse: 0.9246 - val_loss: 0.7273 - val_rmse: 0.8528\n",
      "Epoch 52/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8521 - rmse: 0.9230Epoch 00051: val_rmse improved from 0.85230 to 0.85175, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 9s - loss: 0.8520 - rmse: 0.9230 - val_loss: 0.7255 - val_rmse: 0.8518\n",
      "Epoch 53/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8511 - rmse: 0.9225Epoch 00052: val_rmse did not improve\n",
      "854879/854879 [==============================] - 9s - loss: 0.8512 - rmse: 0.9226 - val_loss: 0.7257 - val_rmse: 0.8519\n",
      "Epoch 54/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8504 - rmse: 0.9221Epoch 00053: val_rmse improved from 0.85175 to 0.85100, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 9s - loss: 0.8504 - rmse: 0.9222 - val_loss: 0.7242 - val_rmse: 0.8510\n",
      "Epoch 55/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8485 - rmse: 0.9211Epoch 00054: val_rmse did not improve\n",
      "854879/854879 [==============================] - 9s - loss: 0.8484 - rmse: 0.9211 - val_loss: 0.7247 - val_rmse: 0.8512\n",
      "Epoch 56/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8479 - rmse: 0.9208Epoch 00055: val_rmse improved from 0.85100 to 0.85085, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 9s - loss: 0.8480 - rmse: 0.9208 - val_loss: 0.7240 - val_rmse: 0.8509\n",
      "Epoch 57/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8458 - rmse: 0.9197Epoch 00056: val_rmse did not improve\n",
      "854879/854879 [==============================] - 9s - loss: 0.8458 - rmse: 0.9197 - val_loss: 0.7245 - val_rmse: 0.8512\n",
      "Epoch 58/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8470 - rmse: 0.9203 ETA: 1s - loss:Epoch 00057: val_rmse improved from 0.85085 to 0.85062, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 9s - loss: 0.8470 - rmse: 0.9203 - val_loss: 0.7236 - val_rmse: 0.8506\n",
      "Epoch 59/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8464 - rmse: 0.9200Epoch 00058: val_rmse did not improve\n",
      "854879/854879 [==============================] - 9s - loss: 0.8464 - rmse: 0.9200 - val_loss: 0.7251 - val_rmse: 0.8515\n",
      "Epoch 60/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8447 - rmse: 0.9190Epoch 00059: val_rmse did not improve\n",
      "854879/854879 [==============================] - 9s - loss: 0.8447 - rmse: 0.9191 - val_loss: 0.7237 - val_rmse: 0.8507\n",
      "Epoch 61/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8439 - rmse: 0.9186Epoch 00060: val_rmse improved from 0.85062 to 0.85044, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 9s - loss: 0.8441 - rmse: 0.9187 - val_loss: 0.7233 - val_rmse: 0.8504\n",
      "Epoch 62/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8441 - rmse: 0.9187Epoch 00061: val_rmse did not improve\n",
      "854879/854879 [==============================] - 9s - loss: 0.8441 - rmse: 0.9187 - val_loss: 0.7238 - val_rmse: 0.8508\n",
      "Epoch 63/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8433 - rmse: 0.9183Epoch 00062: val_rmse improved from 0.85044 to 0.85032, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 9s - loss: 0.8432 - rmse: 0.9183 - val_loss: 0.7231 - val_rmse: 0.8503\n",
      "Epoch 64/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8410 - rmse: 0.9170Epoch 00063: val_rmse improved from 0.85032 to 0.85020, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 9s - loss: 0.8411 - rmse: 0.9171 - val_loss: 0.7229 - val_rmse: 0.8502\n",
      "Epoch 65/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8423 - rmse: 0.9178Epoch 00064: val_rmse did not improve\n",
      "854879/854879 [==============================] - 9s - loss: 0.8426 - rmse: 0.9179 - val_loss: 0.7232 - val_rmse: 0.8504\n",
      "Epoch 66/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8388 - rmse: 0.9158Epoch 00065: val_rmse did not improve\n",
      "854879/854879 [==============================] - 9s - loss: 0.8389 - rmse: 0.9159 - val_loss: 0.7231 - val_rmse: 0.8503\n",
      "Epoch 67/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8386 - rmse: 0.9157Epoch 00066: val_rmse improved from 0.85020 to 0.84996, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 9s - loss: 0.8387 - rmse: 0.9158 - val_loss: 0.7225 - val_rmse: 0.8500\n",
      "Epoch 68/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8389 - rmse: 0.9159Epoch 00067: val_rmse improved from 0.84996 to 0.84966, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 9s - loss: 0.8390 - rmse: 0.9160 - val_loss: 0.7220 - val_rmse: 0.8497\n",
      "Epoch 69/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8371 - rmse: 0.9149Epoch 00068: val_rmse did not improve\n",
      "854879/854879 [==============================] - 10s - loss: 0.8371 - rmse: 0.9149 - val_loss: 0.7221 - val_rmse: 0.8497\n",
      "Epoch 70/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8367 - rmse: 0.9147Epoch 00069: val_rmse did not improve\n",
      "854879/854879 [==============================] - 9s - loss: 0.8367 - rmse: 0.9147 - val_loss: 0.7226 - val_rmse: 0.8501\n",
      "Epoch 71/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8358 - rmse: 0.9142Epoch 00070: val_rmse did not improve\n",
      "854879/854879 [==============================] - 9s - loss: 0.8359 - rmse: 0.9142 - val_loss: 0.7222 - val_rmse: 0.8498\n",
      "Epoch 72/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8350 - rmse: 0.9138Epoch 00071: val_rmse improved from 0.84966 to 0.84922, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 9s - loss: 0.8352 - rmse: 0.9139 - val_loss: 0.7212 - val_rmse: 0.8492\n",
      "Epoch 73/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8345 - rmse: 0.9135Epoch 00072: val_rmse did not improve\n",
      "854879/854879 [==============================] - 8s - loss: 0.8347 - rmse: 0.9136 - val_loss: 0.7220 - val_rmse: 0.8497\n",
      "Epoch 74/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8341 - rmse: 0.9133Epoch 00073: val_rmse improved from 0.84922 to 0.84914, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 8s - loss: 0.8343 - rmse: 0.9134 - val_loss: 0.7211 - val_rmse: 0.8491\n",
      "Epoch 75/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8340 - rmse: 0.9132Epoch 00074: val_rmse did not improve\n",
      "854879/854879 [==============================] - 8s - loss: 0.8341 - rmse: 0.9133 - val_loss: 0.7218 - val_rmse: 0.8496\n",
      "Epoch 76/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8321 - rmse: 0.9122Epoch 00075: val_rmse did not improve\n",
      "854879/854879 [==============================] - 8s - loss: 0.8321 - rmse: 0.9122 - val_loss: 0.7218 - val_rmse: 0.8495\n",
      "Epoch 77/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8327 - rmse: 0.9125Epoch 00076: val_rmse did not improve\n",
      "854879/854879 [==============================] - 8s - loss: 0.8327 - rmse: 0.9125 - val_loss: 0.7215 - val_rmse: 0.8494\n",
      "Epoch 78/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8340 - rmse: 0.9132Epoch 00077: val_rmse improved from 0.84914 to 0.84908, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 8s - loss: 0.8341 - rmse: 0.9133 - val_loss: 0.7210 - val_rmse: 0.8491\n",
      "Epoch 79/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8319 - rmse: 0.9121Epoch 00078: val_rmse improved from 0.84908 to 0.84849, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 8s - loss: 0.8321 - rmse: 0.9122 - val_loss: 0.7200 - val_rmse: 0.8485\n",
      "Epoch 80/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8306 - rmse: 0.9113Epoch 00079: val_rmse did not improve\n",
      "854879/854879 [==============================] - 8s - loss: 0.8307 - rmse: 0.9114 - val_loss: 0.7206 - val_rmse: 0.8489\n",
      "Epoch 81/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8306 - rmse: 0.9114Epoch 00080: val_rmse improved from 0.84849 to 0.84815, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 8s - loss: 0.8306 - rmse: 0.9113 - val_loss: 0.7194 - val_rmse: 0.8481\n",
      "Epoch 82/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8321 - rmse: 0.9122Epoch 00081: val_rmse did not improve\n",
      "854879/854879 [==============================] - 8s - loss: 0.8321 - rmse: 0.9122 - val_loss: 0.7202 - val_rmse: 0.8486\n",
      "Epoch 83/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8286 - rmse: 0.9103Epoch 00082: val_rmse did not improve\n",
      "854879/854879 [==============================] - 8s - loss: 0.8286 - rmse: 0.9102 - val_loss: 0.7204 - val_rmse: 0.8488\n",
      "Epoch 84/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8290 - rmse: 0.9105Epoch 00083: val_rmse did not improve\n",
      "854879/854879 [==============================] - 8s - loss: 0.8290 - rmse: 0.9105 - val_loss: 0.7200 - val_rmse: 0.8485\n",
      "Epoch 85/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8311 - rmse: 0.9116Epoch 00084: val_rmse did not improve\n",
      "854879/854879 [==============================] - 8s - loss: 0.8313 - rmse: 0.9117 - val_loss: 0.7205 - val_rmse: 0.8488\n",
      "Epoch 86/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8299 - rmse: 0.9110Epoch 00085: val_rmse did not improve\n",
      "854879/854879 [==============================] - 8s - loss: 0.8297 - rmse: 0.9109 - val_loss: 0.7200 - val_rmse: 0.8485\n",
      "Epoch 87/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8287 - rmse: 0.9103Epoch 00086: val_rmse did not improve\n",
      "854879/854879 [==============================] - 8s - loss: 0.8289 - rmse: 0.9104 - val_loss: 0.7205 - val_rmse: 0.8488\n",
      "Epoch 88/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8272 - rmse: 0.9095Epoch 00087: val_rmse did not improve\n",
      "854879/854879 [==============================] - 8s - loss: 0.8273 - rmse: 0.9095 - val_loss: 0.7198 - val_rmse: 0.8484\n",
      "Epoch 89/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8274 - rmse: 0.9096Epoch 00088: val_rmse did not improve\n",
      "854879/854879 [==============================] - 8s - loss: 0.8273 - rmse: 0.9096 - val_loss: 0.7210 - val_rmse: 0.8491\n",
      "Epoch 90/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8285 - rmse: 0.9102Epoch 00089: val_rmse did not improve\n",
      "854879/854879 [==============================] - 8s - loss: 0.8284 - rmse: 0.9102 - val_loss: 0.7200 - val_rmse: 0.8485\n",
      "Epoch 91/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8265 - rmse: 0.9091Epoch 00090: val_rmse did not improve\n",
      "854879/854879 [==============================] - 8s - loss: 0.8264 - rmse: 0.9090 - val_loss: 0.7201 - val_rmse: 0.8486\n",
      "Epoch 92/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8266 - rmse: 0.9092Epoch 00091: val_rmse did not improve\n",
      "854879/854879 [==============================] - 8s - loss: 0.8266 - rmse: 0.9092 - val_loss: 0.7200 - val_rmse: 0.8485\n",
      "Epoch 93/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8248 - rmse: 0.9082 ETA: 1s -Epoch 00092: val_rmse did not improve\n",
      "854879/854879 [==============================] - 8s - loss: 0.8248 - rmse: 0.9082 - val_loss: 0.7200 - val_rmse: 0.8485\n",
      "Epoch 94/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8253 - rmse: 0.9084Epoch 00093: val_rmse did not improve\n",
      "854879/854879 [==============================] - 8s - loss: 0.8253 - rmse: 0.9084 - val_loss: 0.7200 - val_rmse: 0.8485\n",
      "Epoch 95/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8251 - rmse: 0.9083Epoch 00094: val_rmse did not improve\n",
      "854879/854879 [==============================] - 8s - loss: 0.8253 - rmse: 0.9084 - val_loss: 0.7198 - val_rmse: 0.8484\n",
      "Epoch 96/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8248 - rmse: 0.9081Epoch 00095: val_rmse improved from 0.84815 to 0.84795, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 8s - loss: 0.8248 - rmse: 0.9081 - val_loss: 0.7190 - val_rmse: 0.8480\n",
      "Epoch 97/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8254 - rmse: 0.9085Epoch 00096: val_rmse did not improve\n",
      "854879/854879 [==============================] - 8s - loss: 0.8253 - rmse: 0.9085 - val_loss: 0.7195 - val_rmse: 0.8482\n",
      "Epoch 98/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8258 - rmse: 0.9087Epoch 00097: val_rmse did not improve\n",
      "854879/854879 [==============================] - 8s - loss: 0.8259 - rmse: 0.9088 - val_loss: 0.7200 - val_rmse: 0.8485\n",
      "Epoch 99/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8247 - rmse: 0.9081Epoch 00098: val_rmse did not improve\n",
      "854879/854879 [==============================] - 8s - loss: 0.8247 - rmse: 0.9081 - val_loss: 0.7195 - val_rmse: 0.8482\n",
      "Epoch 100/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8248 - rmse: 0.9082Epoch 00099: val_rmse did not improve\n",
      "854879/854879 [==============================] - 8s - loss: 0.8249 - rmse: 0.9082 - val_loss: 0.7202 - val_rmse: 0.8486\n",
      "Epoch 101/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8230 - rmse: 0.9072Epoch 00100: val_rmse improved from 0.84795 to 0.84793, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 8s - loss: 0.8230 - rmse: 0.9072 - val_loss: 0.7190 - val_rmse: 0.8479\n",
      "Epoch 102/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8240 - rmse: 0.9077Epoch 00101: val_rmse did not improve\n",
      "854879/854879 [==============================] - 8s - loss: 0.8240 - rmse: 0.9077 - val_loss: 0.7195 - val_rmse: 0.8482\n",
      "Epoch 103/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8218 - rmse: 0.9065Epoch 00102: val_rmse did not improve\n",
      "854879/854879 [==============================] - 8s - loss: 0.8218 - rmse: 0.9065 - val_loss: 0.7192 - val_rmse: 0.8480\n",
      "Epoch 104/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8229 - rmse: 0.9071Epoch 00103: val_rmse did not improve\n",
      "854879/854879 [==============================] - 8s - loss: 0.8229 - rmse: 0.9071 - val_loss: 0.7190 - val_rmse: 0.8479\n",
      "Epoch 105/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8209 - rmse: 0.9060Epoch 00104: val_rmse improved from 0.84793 to 0.84790, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 8s - loss: 0.8207 - rmse: 0.9059 - val_loss: 0.7189 - val_rmse: 0.8479\n",
      "Epoch 106/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8222 - rmse: 0.9067Epoch 00105: val_rmse did not improve\n",
      "854879/854879 [==============================] - 8s - loss: 0.8223 - rmse: 0.9068 - val_loss: 0.7197 - val_rmse: 0.8484\n",
      "Epoch 107/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8226 - rmse: 0.9070Epoch 00106: val_rmse did not improve\n",
      "854879/854879 [==============================] - 8s - loss: 0.8229 - rmse: 0.9071 - val_loss: 0.7203 - val_rmse: 0.8487\n",
      "Epoch 108/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8225 - rmse: 0.9069Epoch 00107: val_rmse did not improve\n",
      "854879/854879 [==============================] - 8s - loss: 0.8226 - rmse: 0.9069 - val_loss: 0.7195 - val_rmse: 0.8482\n",
      "Epoch 109/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8228 - rmse: 0.9070Epoch 00108: val_rmse did not improve\n",
      "854879/854879 [==============================] - 8s - loss: 0.8230 - rmse: 0.9071 - val_loss: 0.7207 - val_rmse: 0.8489\n",
      "Epoch 110/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8195 - rmse: 0.9052Epoch 00109: val_rmse did not improve\n",
      "854879/854879 [==============================] - 8s - loss: 0.8195 - rmse: 0.9052 - val_loss: 0.7191 - val_rmse: 0.8480\n",
      "Epoch 111/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8213 - rmse: 0.9063Epoch 00110: val_rmse did not improve\n",
      "854879/854879 [==============================] - 7s - loss: 0.8215 - rmse: 0.9064 - val_loss: 0.7200 - val_rmse: 0.8485\n",
      "Epoch 112/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8217 - rmse: 0.9064Epoch 00111: val_rmse did not improve\n",
      "854879/854879 [==============================] - 7s - loss: 0.8216 - rmse: 0.9064 - val_loss: 0.7197 - val_rmse: 0.8483\n",
      "Epoch 113/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8196 - rmse: 0.9053Epoch 00112: val_rmse did not improve\n",
      "854879/854879 [==============================] - 8s - loss: 0.8195 - rmse: 0.9053 - val_loss: 0.7201 - val_rmse: 0.8486\n",
      "Epoch 114/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8195 - rmse: 0.9052Epoch 00113: val_rmse did not improve\n",
      "854879/854879 [==============================] - 7s - loss: 0.8195 - rmse: 0.9053 - val_loss: 0.7195 - val_rmse: 0.8482\n",
      "Epoch 115/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8195 - rmse: 0.9052Epoch 00114: val_rmse improved from 0.84790 to 0.84772, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 8s - loss: 0.8195 - rmse: 0.9052 - val_loss: 0.7187 - val_rmse: 0.8477\n",
      "Epoch 116/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8214 - rmse: 0.9063Epoch 00115: val_rmse did not improve\n",
      "854879/854879 [==============================] - 8s - loss: 0.8214 - rmse: 0.9063 - val_loss: 0.7187 - val_rmse: 0.8478\n",
      "Epoch 117/500\n",
      "850000/854879 [============================>.] - ETA: 1s - loss: 0.8189 - rmse: 0.9049Epoch 00116: val_rmse did not improve\n",
      "854879/854879 [==============================] - 261s - loss: 0.8190 - rmse: 0.9049 - val_loss: 0.7196 - val_rmse: 0.8483\n",
      "Epoch 118/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8170 - rmse: 0.9038Epoch 00117: val_rmse improved from 0.84772 to 0.84744, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 11s - loss: 0.8169 - rmse: 0.9038 - val_loss: 0.7182 - val_rmse: 0.8474\n",
      "Epoch 119/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8173 - rmse: 0.9040Epoch 00118: val_rmse did not improve\n",
      "854879/854879 [==============================] - 11s - loss: 0.8172 - rmse: 0.9040 - val_loss: 0.7192 - val_rmse: 0.8481\n",
      "Epoch 120/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8201 - rmse: 0.9056Epoch 00119: val_rmse did not improve\n",
      "854879/854879 [==============================] - 9s - loss: 0.8201 - rmse: 0.9056 - val_loss: 0.7187 - val_rmse: 0.8478\n",
      "Epoch 121/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8188 - rmse: 0.9048 ETA: 1s -Epoch 00120: val_rmse did not improve\n",
      "854879/854879 [==============================] - 10s - loss: 0.8190 - rmse: 0.9050 - val_loss: 0.7188 - val_rmse: 0.8478\n",
      "Epoch 122/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8174 - rmse: 0.9041Epoch 00121: val_rmse did not improve\n",
      "854879/854879 [==============================] - 11s - loss: 0.8175 - rmse: 0.9041 - val_loss: 0.7189 - val_rmse: 0.8479\n",
      "Epoch 123/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8169 - rmse: 0.9038Epoch 00122: val_rmse improved from 0.84744 to 0.84720, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 12s - loss: 0.8170 - rmse: 0.9039 - val_loss: 0.7178 - val_rmse: 0.8472\n",
      "Epoch 124/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8166 - rmse: 0.9037Epoch 00123: val_rmse did not improve\n",
      "854879/854879 [==============================] - 10s - loss: 0.8166 - rmse: 0.9036 - val_loss: 0.7180 - val_rmse: 0.8474\n",
      "Epoch 125/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8164 - rmse: 0.9035Epoch 00124: val_rmse did not improve\n",
      "854879/854879 [==============================] - 9s - loss: 0.8165 - rmse: 0.9036 - val_loss: 0.7195 - val_rmse: 0.8482\n",
      "Epoch 126/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8188 - rmse: 0.9048Epoch 00125: val_rmse did not improve\n",
      "854879/854879 [==============================] - 9s - loss: 0.8187 - rmse: 0.9048 - val_loss: 0.7188 - val_rmse: 0.8478\n",
      "Epoch 127/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8182 - rmse: 0.9045Epoch 00126: val_rmse did not improve\n",
      "854879/854879 [==============================] - 8s - loss: 0.8183 - rmse: 0.9046 - val_loss: 0.7194 - val_rmse: 0.8482\n",
      "Epoch 128/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8179 - rmse: 0.9043 ETA: 0s - loss: 0.8174 - rmseEpoch 00127: val_rmse did not improve\n",
      "854879/854879 [==============================] - 9s - loss: 0.8179 - rmse: 0.9044 - val_loss: 0.7186 - val_rmse: 0.8477\n",
      "Epoch 129/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8171 - rmse: 0.9039Epoch 00128: val_rmse did not improve\n",
      "854879/854879 [==============================] - 9s - loss: 0.8172 - rmse: 0.9040 - val_loss: 0.7185 - val_rmse: 0.8477\n",
      "Epoch 130/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8177 - rmse: 0.9042Epoch 00129: val_rmse did not improve\n",
      "854879/854879 [==============================] - 9s - loss: 0.8179 - rmse: 0.9043 - val_loss: 0.7182 - val_rmse: 0.8474\n",
      "Epoch 131/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8161 - rmse: 0.9034Epoch 00130: val_rmse did not improve\n",
      "854879/854879 [==============================] - 9s - loss: 0.8160 - rmse: 0.9033 - val_loss: 0.7181 - val_rmse: 0.8474\n",
      "Epoch 132/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8170 - rmse: 0.9039Epoch 00131: val_rmse did not improve\n",
      "854879/854879 [==============================] - 9s - loss: 0.8172 - rmse: 0.9040 - val_loss: 0.7184 - val_rmse: 0.8476\n",
      "Epoch 133/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8165 - rmse: 0.9036Epoch 00132: val_rmse did not improve\n",
      "854879/854879 [==============================] - 9s - loss: 0.8167 - rmse: 0.9037 - val_loss: 0.7183 - val_rmse: 0.8475\n",
      "Epoch 134/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8168 - rmse: 0.9037Epoch 00133: val_rmse did not improve\n",
      "854879/854879 [==============================] - 9s - loss: 0.8168 - rmse: 0.9037 - val_loss: 0.7182 - val_rmse: 0.8474\n",
      "Epoch 135/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8157 - rmse: 0.9031Epoch 00134: val_rmse improved from 0.84720 to 0.84704, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 9s - loss: 0.8158 - rmse: 0.9032 - val_loss: 0.7175 - val_rmse: 0.8470\n",
      "Epoch 136/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8166 - rmse: 0.9036Epoch 00135: val_rmse improved from 0.84704 to 0.84665, saving model to hw5_nobias.hdf5\n",
      "854879/854879 [==============================] - 9s - loss: 0.8167 - rmse: 0.9037 - val_loss: 0.7168 - val_rmse: 0.8466\n",
      "Epoch 137/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8163 - rmse: 0.9034Epoch 00136: val_rmse did not improve\n",
      "854879/854879 [==============================] - 9s - loss: 0.8164 - rmse: 0.9035 - val_loss: 0.7179 - val_rmse: 0.8473\n",
      "Epoch 138/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8159 - rmse: 0.9032Epoch 00137: val_rmse did not improve\n",
      "854879/854879 [==============================] - 9s - loss: 0.8160 - rmse: 0.9033 - val_loss: 0.7170 - val_rmse: 0.8468\n",
      "Epoch 139/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8150 - rmse: 0.9027Epoch 00138: val_rmse did not improve\n",
      "854879/854879 [==============================] - 9s - loss: 0.8150 - rmse: 0.9027 - val_loss: 0.7184 - val_rmse: 0.8475\n",
      "Epoch 140/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8165 - rmse: 0.9036Epoch 00139: val_rmse did not improve\n",
      "854879/854879 [==============================] - 9s - loss: 0.8165 - rmse: 0.9036 - val_loss: 0.7182 - val_rmse: 0.8475\n",
      "Epoch 141/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8134 - rmse: 0.9019Epoch 00140: val_rmse did not improve\n",
      "854879/854879 [==============================] - 9s - loss: 0.8134 - rmse: 0.9019 - val_loss: 0.7177 - val_rmse: 0.8472\n",
      "Epoch 142/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8163 - rmse: 0.9035Epoch 00141: val_rmse did not improve\n",
      "854879/854879 [==============================] - 8s - loss: 0.8161 - rmse: 0.9034 - val_loss: 0.7187 - val_rmse: 0.8478\n",
      "Epoch 143/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8159 - rmse: 0.9032Epoch 00142: val_rmse did not improve\n",
      "854879/854879 [==============================] - 9s - loss: 0.8159 - rmse: 0.9033 - val_loss: 0.7174 - val_rmse: 0.8470\n",
      "Epoch 144/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8151 - rmse: 0.9028Epoch 00143: val_rmse did not improve\n",
      "854879/854879 [==============================] - 9s - loss: 0.8152 - rmse: 0.9029 - val_loss: 0.7177 - val_rmse: 0.8472\n",
      "Epoch 145/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8147 - rmse: 0.9026Epoch 00144: val_rmse did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "854879/854879 [==============================] - 9s - loss: 0.8146 - rmse: 0.9025 - val_loss: 0.7169 - val_rmse: 0.8467\n",
      "Epoch 146/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8145 - rmse: 0.9025Epoch 00145: val_rmse did not improve\n",
      "854879/854879 [==============================] - 9s - loss: 0.8146 - rmse: 0.9026 - val_loss: 0.7178 - val_rmse: 0.8472\n",
      "Epoch 147/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8145 - rmse: 0.9025Epoch 00146: val_rmse did not improve\n",
      "854879/854879 [==============================] - 9s - loss: 0.8147 - rmse: 0.9026 - val_loss: 0.7176 - val_rmse: 0.8471\n",
      "Epoch 148/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8136 - rmse: 0.9020Epoch 00147: val_rmse did not improve\n",
      "854879/854879 [==============================] - 9s - loss: 0.8137 - rmse: 0.9020 - val_loss: 0.7192 - val_rmse: 0.8480\n",
      "Epoch 149/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8150 - rmse: 0.9028Epoch 00148: val_rmse did not improve\n",
      "854879/854879 [==============================] - 9s - loss: 0.8151 - rmse: 0.9028 - val_loss: 0.7181 - val_rmse: 0.8474\n",
      "Epoch 150/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8156 - rmse: 0.9031Epoch 00149: val_rmse did not improve\n",
      "854879/854879 [==============================] - 9s - loss: 0.8157 - rmse: 0.9031 - val_loss: 0.7192 - val_rmse: 0.8481\n",
      "Epoch 151/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8130 - rmse: 0.9016Epoch 00150: val_rmse did not improve\n",
      "854879/854879 [==============================] - 9s - loss: 0.8131 - rmse: 0.9017 - val_loss: 0.7192 - val_rmse: 0.8481\n",
      "Epoch 152/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8148 - rmse: 0.9026Epoch 00151: val_rmse did not improve\n",
      "854879/854879 [==============================] - 9s - loss: 0.8148 - rmse: 0.9027 - val_loss: 0.7172 - val_rmse: 0.8469\n",
      "Epoch 153/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8142 - rmse: 0.9023Epoch 00152: val_rmse did not improve\n",
      "854879/854879 [==============================] - 10s - loss: 0.8140 - rmse: 0.9022 - val_loss: 0.7183 - val_rmse: 0.8475\n",
      "Epoch 154/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8131 - rmse: 0.9017Epoch 00153: val_rmse did not improve\n",
      "854879/854879 [==============================] - 9s - loss: 0.8131 - rmse: 0.9017 - val_loss: 0.7182 - val_rmse: 0.8475\n",
      "Epoch 155/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8130 - rmse: 0.9016Epoch 00154: val_rmse did not improve\n",
      "854879/854879 [==============================] - 9s - loss: 0.8132 - rmse: 0.9017 - val_loss: 0.7178 - val_rmse: 0.8472\n",
      "Epoch 156/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8136 - rmse: 0.9020Epoch 00155: val_rmse did not improve\n",
      "854879/854879 [==============================] - 10s - loss: 0.8136 - rmse: 0.9020 - val_loss: 0.7181 - val_rmse: 0.8474\n",
      "Epoch 157/500\n",
      "850000/854879 [============================>.] - ETA: 0s - loss: 0.8133 - rmse: 0.9018Epoch 00156: val_rmse did not improve\n",
      "854879/854879 [==============================] - 9s - loss: 0.8134 - rmse: 0.9018 - val_loss: 0.7182 - val_rmse: 0.8475\n",
      "Epoch 00156: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x105d07940>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss = 'mse', optimizer='adam', metrics=[rmse])\n",
    "filepath = 'hw5_nobias.hdf5'\n",
    "callbacks = [EarlyStopping(monitor='val_rmse', patience=20, verbose=1, mode='min'), \n",
    "             ModelCheckpoint(filepath, monitor='val_rmse', save_best_only=True, verbose=1, mode='min')]\n",
    "model.fit([user, movie], rating, epochs=500, batch_size=10000, validation_split=0.05, verbose=1, callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100336\n"
     ]
    }
   ],
   "source": [
    "text = open('test.csv', 'r') \n",
    "row = csv.reader(text , delimiter=\"\\n\")\n",
    "count = 0\n",
    "user_pred = []\n",
    "movie_pred = []\n",
    "for r in row:\n",
    "    if count > 0:\n",
    "        r = r[0].split(',')\n",
    "        user_pred.append(r[1])\n",
    "        movie_pred.append(r[2])\n",
    "    count = 1\n",
    "text.close()\n",
    "print(len(user_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100336\n"
     ]
    }
   ],
   "source": [
    "text = open('test.csv', 'r') \n",
    "row = csv.reader(text , delimiter=\"\\n\")\n",
    "count = 0\n",
    "user_pred = []\n",
    "movie_pred = []\n",
    "genres_pred = []\n",
    "for r in row:\n",
    "    if count > 0:\n",
    "        r = r[0].split(',')\n",
    "        user_pred.append(r[1])\n",
    "        movie_pred.append(r[2])\n",
    "        #genres_pred.append(m_genres[int(r[2])-1])\n",
    "    count = 1\n",
    "text.close()\n",
    "print(len(user_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6040\n",
      "3952\n"
     ]
    }
   ],
   "source": [
    "user_pred = np.array(user_pred)\n",
    "movie_pred = np.array(movie_pred)\n",
    "\n",
    "user_pred=user_pred.astype(np.int)\n",
    "movie_pred=movie_pred.astype(np.int)\n",
    "\n",
    "#genres_pred = np.array(genres_pred)\n",
    "#genres_pred=genres_pred.astype(np.int)\n",
    "\n",
    "max_user_pred = np.max(user_pred)\n",
    "max_movie_pred = np.max(movie_pred)\n",
    "print(max_user_pred)\n",
    "print(max_movie_pred)\n",
    "#K_FACTORS = 120\n",
    "\n",
    "user_pred = user_pred - 1\n",
    "movie_pred = movie_pred - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100336\n"
     ]
    }
   ],
   "source": [
    "#from keras.models import load_model\n",
    "model_pred = load_model('hw5_nobias.hdf5', custom_objects={'rmse': rmse})\n",
    "predict = model_pred.predict([user_pred, movie_pred])\n",
    "#predict = []\n",
    "#for i in range(len(user_pred)):\n",
    "#    predict.append(model.rate(user_pred[i], movie_pred[i]))\n",
    "print(len(predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#normalize\n",
    "#predict = predict * std + means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict = np.clip(predict, 1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ans = []\n",
    "for i in range(len(predict)):\n",
    "    ans.append([str(i+1)])\n",
    "    #a = float(round(predict[i][0]))\n",
    "    a = float(predict[i][0])\n",
    "    ans[i].append(a)\n",
    "\n",
    "filename = 'hw5_nobias.csv'\n",
    "text = open(filename, \"w+\")\n",
    "s = csv.writer(text,delimiter=',',lineterminator='\\n')\n",
    "s.writerow(['TestDataID','Rating'])\n",
    "for i in range(len(ans)):\n",
    "    s.writerow(ans[i]) \n",
    "text.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
